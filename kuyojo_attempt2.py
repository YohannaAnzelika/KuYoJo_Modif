# -*- coding: utf-8 -*-
"""KuYoJo_Attempt2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GTeEs8nj1TR9MahRZXU3F50xQzP8GjII

# üìö Tugas Deep Learning
## ‚ú® Judul: Perbandingan Plain34.py dan ResNet34

---

### üë• Kelompok
- **Nama Kelompok**: KuYoJo

### üë§ Anggota:
1. Yohanna Anzelika Sitepu (122140010)  
2. Kayla Chika Lathisya (122140009)  
3. Joy Daniella (122140039)  


---

üìÖ **Semester**: Ganjil 2025  
üè´ **Institut Teknologi Sumatera (ITERA)**

# **Import library**
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image

from google.colab import drive
drive.mount('/content/drive')

import os

# Gunakan os.path.join supaya aman walau ada spasi
base_dir = "/content/drive/MyDrive/Dataset Tugas"

train_csv = os.path.join(base_dir, "train.csv")
train_root = os.path.join(base_dir, "train")

"""# **Path dataset**"""

train_csv = "/content/drive/MyDrive/Dataset Tugas/train.csv"
train_root = "/content/drive/MyDrive/Dataset Tugas/train/"

"""# **Transformasi gambar**"""

transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406],
                         std=[0.229,0.224,0.225])
])

"""# **Dataset class**"""

class FoodDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None):
        self.data = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform
        self.classes = sorted(self.data.iloc[:,1].unique())
        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_name = self.data.iloc[idx, 0]
        label_name = self.data.iloc[idx, 1]
        label = self.class_to_idx[label_name]
        img_path = os.path.join(self.root_dir, img_name)

        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, label

"""# **Bersihkan CSV (hapus baris file hilang)**"""

full_df = pd.read_csv(train_csv)
full_df = full_df[full_df.iloc[:,0].apply(lambda x: os.path.exists(os.path.join(train_root, x)))]
full_df = full_df.reset_index(drop=True)

print("Jumlah data setelah dibersihkan:", len(full_df))

# Split train/val
train_df, val_df = train_test_split(
    full_df, test_size=0.2,
    stratify=full_df.iloc[:,1],
    random_state=42
)

train_df.to_csv("train_split.csv", index=False)
val_df.to_csv("val_split.csv", index=False)

# Dataset & DataLoader
train_ds = FoodDataset("train_split.csv", train_root, transform=transform)
val_ds   = FoodDataset("val_split.csv", train_root, transform=transform)

train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)
val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False)

num_classes = len(train_ds.classes)
print("Train size:", len(train_ds))
print("Val size:", len(val_ds))
print("Kelas:", train_ds.classes)

"""# **Model Plain34 & ResNet34**"""

# ---- Plain34 (tanpa residual) ----
class PlainBlock(nn.Module):
    def __init__(self,in_ch,out_ch,stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch,out_ch,3,stride,1,bias=False)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_ch,out_ch,3,1,1,bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch)
    def forward(self,x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        return out

class Plain34(nn.Module):
    def __init__(self,num_classes=5):
        super().__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(3,64,7,2,3,bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3,2,1)
        )
        self.layer2 = self._make_layer(64,64,3)
        self.layer3 = self._make_layer(64,128,4,stride=2)
        self.layer4 = self._make_layer(128,256,6,stride=2)
        self.layer5 = self._make_layer(256,512,3,stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(512,num_classes)

    def _make_layer(self,in_ch,out_ch,blocks,stride=1):
        layers=[PlainBlock(in_ch,out_ch,stride)]
        for _ in range(1,blocks):
            layers.append(PlainBlock(out_ch,out_ch))
        return nn.Sequential(*layers)

    def forward(self,x):
        x=self.layer1(x)
        x=self.layer2(x)
        x=self.layer3(x)
        x=self.layer4(x)
        x=self.layer5(x)
        x=self.avgpool(x)
        x=torch.flatten(x,1)
        x=self.fc(x)
        return x

def create_plain34(num_classes):
    return Plain34(num_classes=num_classes)

# ---- ResNet34 (dengan residual) ----
class ResidualBlock(nn.Module):
    def __init__(self,in_ch,out_ch,stride=1,downsample=None):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch,out_ch,3,stride,1,bias=False)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_ch,out_ch,3,1,1,bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch)
        self.downsample = downsample
    def forward(self,x):
        identity = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.downsample:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out

class ResNet34(nn.Module):
    def __init__(self,num_classes=5):
        super().__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(3,64,7,2,3,bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(3,2,1)
        )
        self.layer2 = self._make_layer(64,64,3)
        self.layer3 = self._make_layer(64,128,4,stride=2)
        self.layer4 = self._make_layer(128,256,6,stride=2)
        self.layer5 = self._make_layer(256,512,3,stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(512,num_classes)

    def _make_layer(self,in_ch,out_ch,blocks,stride=1):
        downsample=None
        if stride!=1 or in_ch!=out_ch:
            downsample=nn.Sequential(
                nn.Conv2d(in_ch,out_ch,1,stride,bias=False),
                nn.BatchNorm2d(out_ch)
            )
        layers=[ResidualBlock(in_ch,out_ch,stride,downsample)]
        for _ in range(1,blocks):
            layers.append(ResidualBlock(out_ch,out_ch))
        return nn.Sequential(*layers)

    def forward(self,x):
        x=self.layer1(x)
        x=self.layer2(x)
        x=self.layer3(x)
        x=self.layer4(x)
        x=self.layer5(x)
        x=self.avgpool(x)
        x=torch.flatten(x,1)
        x=self.fc(x)
        return x

def create_resnet34(num_classes):
    return ResNet34(num_classes=num_classes)

"""# **Training function**"""

def train_model(model, train_loader, val_loader, device, epochs=5, lr=0.001):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    model.to(device)

    history = []
    for epoch in range(1, epochs+1):
        # --- Training ---
        model.train()
        train_loss, correct, total = 0,0,0
        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()*imgs.size(0)
            _, preds = outputs.max(1)
            correct += preds.eq(labels).sum().item()
            total += labels.size(0)
        train_acc = correct/total
        train_loss /= total

        # --- Validation ---
        model.eval()
        val_loss, correct, total = 0,0,0
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device)
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()*imgs.size(0)
                _, preds = outputs.max(1)
                correct += preds.eq(labels).sum().item()
                total += labels.size(0)
        val_acc = correct/total
        val_loss /= total

        print(f"Epoch {epoch}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}")
        history.append([epoch, train_loss, train_acc, val_loss, val_acc])
    return history

"""# **Train Plain34 & ResNet34**"""

# Pilih device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on:", device)


def train_model(model, train_loader, val_loader, device, epochs=5, lr=0.001, model_name="Model"):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    model.to(device)

    print(f"\n=== Training {model_name} ===")
    history = []
    best_val_acc = 0.0

    for epoch in range(1, epochs+1):
        # --- Training ---
        model.train()
        train_loss, correct, total = 0, 0, 0
        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * imgs.size(0)
            _, preds = outputs.max(1)
            correct += preds.eq(labels).sum().item()
            total += labels.size(0)
        train_acc = correct / total
        train_loss /= total

        # --- Validation ---
        model.eval()
        val_loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device)
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * imgs.size(0)
                _, preds = outputs.max(1)
                correct += preds.eq(labels).sum().item()
                total += labels.size(0)
        val_acc = correct / total
        val_loss /= total

        if val_acc > best_val_acc:
            best_val_acc = val_acc

        print(f"Epoch {epoch:02d}: Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}")
        history.append([epoch, train_loss, train_acc, val_loss, val_acc])

    print(f"Best Val Acc for {model_name}: {best_val_acc:.4f}")
    return history


# --- Train Plain34 (baseline) ---
plain_model = create_plain34(num_classes=num_classes)
plain_hist = train_model(
    plain_model,
    train_loader,
    val_loader,
    device,
    epochs=20,
    model_name="Plain34 (baseline)"
)

# --- Train ResNet34 (dengan residual) ---
resnet_model = create_resnet34(num_classes=num_classes)
resnet_hist = train_model(
    resnet_model,
    train_loader,
    val_loader,
    device,
    epochs=20,
    model_name="ResNet34 (dengan residual)"
)

"""# **ResNet34_Modified**"""

class ResidualBlock_Modified(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1, downsample=None, p=0.3):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.act = nn.Mish()
        self.dropout = nn.Dropout(p)
        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        out = self.act(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.dropout(self.act(out))
        return out


class ResNet34_Modified(nn.Module):
    def __init__(self, num_classes=5):
        super().__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.Mish(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        self.layer2 = self._make_layer(64, 64, 3)
        self.layer3 = self._make_layer(64, 128, 4, stride=2)
        self.layer4 = self._make_layer(128, 256, 6, stride=2)
        self.layer5 = self._make_layer(256, 512, 3, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, in_ch, out_ch, blocks, stride=1):
        downsample = None
        if stride != 1 or in_ch != out_ch:
            downsample = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_ch)
            )
        layers = [ResidualBlock_Modified(in_ch, out_ch, stride, downsample)]
        for _ in range(1, blocks):
            layers.append(ResidualBlock_Modified(out_ch, out_ch))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.layer5(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x


def create_resnet34_modified(num_classes):
    return ResNet34_Modified(num_classes=num_classes)


# === Training ResNet34_Modified ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on:", device)

resnet_mod = create_resnet34_modified(num_classes=num_classes)
resnet_mod_hist = train_model(
    resnet_mod,
    train_loader,
    val_loader,
    device,
    epochs=20,
    model_name="ResNet34_Modified (Mish + Dropout)"
)

"""# **Plot hasil**"""

plot_history_all(plain_hist, resnet_hist, resnet_mod_hist)

"""# **Save History**"""

# === Save History ResNet34_Modified ===
resnet_mod_df = pd.DataFrame(resnet_mod_hist, columns=["epoch","train_loss","train_acc","val_loss","val_acc"])
resnet_mod_df.to_csv("/content/drive/MyDrive/Dataset Tugas/savehistory_resnet34_modified.csv", index=False)
print("\nTraining ResNet34_Modified complete!")
best_resnet_mod_acc = max(resnet_mod_df["val_acc"])*100
print(f"Best Validation Accuracy (ResNet34_Modified): {best_resnet_mod_acc:.2f}%")